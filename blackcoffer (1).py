# -*- coding: utf-8 -*-
"""BlackCoffer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bXQFksu69Ll1RyfNW5Jpcy4rBf7Pc585
"""

import os
os.makedirs('/Blackcoffer/Assingment')

import pandas as pd
import requests
from bs4 import BeautifulSoup

# read the input.xlsx file
data = pd.read_csv('/Blackcoffer/Assignment/Input.csv')
text_files=[]
# iterate through each row of the dataframe
for index, row in data.iterrows():
    # extract the URL and URL_ID from the current row
    url = row['URL']
    url_id = row['URL_ID']
    
    # get the webpage content
    response = requests.get(url)
    
    # parse the HTML content using BeautifulSoup
    soup = BeautifulSoup(response.content, 'html.parser')
    
    # find the article text and title
    article_text = ''
    article_title = ''
    article = soup.find('article')
    if article:
        # extract the article text
        article_text = article.get_text().strip()
        
        # extract the article title
        article_title = article.find('h1').get_text().strip()
    
    with open(f'/Blackcoffer/Assignment/{url_id}.txt', 'w') as f:
        f.write(f'{article_title}\n\n{article_text}')

    with open(f'/Blackcoffer/Assignment/{url_id}.txt','r')as file:
      text_x=file.read()
    text_files.append(text_x)

# creating list of all stop words
def stopwords(stopwords_file):
  file_name='{}'.format(stopwords_file)
  with open(file_name,'r') as file:
    stopwords=file.read()
    stopwords=stopwords.split('\n')
   
  return stopwords

list_stopwords_fil_names=['/Blackcoffer/Assignment/StopWords_Auditor.txt','/Blackcoffer/Assignment/StopWords_Currencies.txt','/Blackcoffer/Assignment/StopWords_DatesandNumbers.txt','/Blackcoffer/Assignment/StopWords_Generic.txt','/Blackcoffer/Assignment/StopWords_GenericLong.txt','/Blackcoffer/Assignment/StopWords_Geographic.txt','/Blackcoffer/Assignment/StopWords_Names.txt']

# making two funs for read files  and return list of stop words
def stopwords_curr(file_name):
  with open(file_name, 'r', encoding='ISO-8859-1') as f:
    file=f.read()
    
  file=file.split(' \n')
  currency_country_pairs=[line.strip().split(' | ') for line in file]
  #print(currency_country_pairs)
  stopwards_currencies= []
  for sublist in currency_country_pairs:
    for item in sublist:
      stopwards_currencies.append(item)
  
  return stopwards_currencies

def all_stopwords(list_stopwords_fil_names):
  stopwords_all=[]
  for file_name in list_stopwords_fil_names:
    try:
      for word in stopwords(file_name):
        stopwords_all.append(word.upper())
      print('{}'.format(file_name)+' is working for the stopwords')
    except:
      for word1 in stopwords_curr(file_name):
        stopwords_all.append(word1.upper())
      print('{}'.format(file_name)+' is working for the stopwords_curr')
      
  return stopwords_all

# creating all stop words list using all stopwords fun
list_stopwords=all_stopwords(list_stopwords_fil_names)

import re
# creating remove function to remove all stop words using all stop words list
def remove_stopwords(text_file,list_stopwords):
  # Split the text into words
  words = text_file.split()
  # Remove stopwords from the text
  filtered_words = [word for word in words if word not in list_stopwords]
  # Join the filtered words to form the new text
  text_file = " ".join(filtered_words)

  return text_file

text_filex=[]
# removing all stopwords present in each text of text files
for text in text_files:
  text=text.upper()
  text_filex.append(remove_stopwords(text,list_stopwords))

positive_words = {}
negative_words = {}
# getting all positive words in positive_words dictionary
with open('/Blackcoffer/Assignment/positive-words.txt') as f:
    for line in f:
        word = line.strip().upper()
        if word not in list_stopwords:
            positive_words[word] = True
# getting all positive words in negative_words dictionary
with open('/Blackcoffer/Assignment/negative-words.txt',encoding='latin-1') as f:
    for line in f:
        word = line.strip().upper()
        if word not in list_stopwords:
            negative_words[word] = True

import nltk
from nltk.tokenize import word_tokenize,sent_tokenize
nltk.download('punkt')
def tokenizer(text_file):
  # creating token from text file
  tokens = word_tokenize(text_file.upper())
  return tokens

text_filex_tokens=[tokenizer(text) for text in text_filex]

def variables(tokens):
  # Calculate the positive and negative scores
  positive_score = sum(1 for token in tokens if token.upper() in positive_words)
  negative_score = -1*(sum(-1 for token in tokens if token.upper() in negative_words))
  # Calculate the polarity and subjectivity scores
  total_words_after_cleaning=len(tokens)
  polarity_score =(positive_score - negative_score)/((positive_score + negative_score) + 0.000001)
  subjectivity_score =(positive_score + negative_score)/((total_words_after_cleaning) + 0.000001)
  return ( positive_score, negative_score, polarity_score, subjectivity_score)

def sent_tokenizer(text_file):
  #creating tokken from fun sent tokenizer
  tokens = sent_tokenize(text_file.upper())
  return tokens

text_filex_sent_tokens=[sent_tokenizer(text) for text in text_filex]

import re

def complex_word_count(text_file):
    # Define a regex pattern to match words with more than two syllables
    pattern = r'\b\w*(?:[aeiouyAEIOUY][^aeiouyAEIOUY]*){3,}\w*\b'
    
    # Find all matches in the text
    matches = re.findall(pattern, text_file)
    
    # Return the count of matches
    return len(matches)

def analysis_of_readibility(text_file):
  # getting sentence lengths in list 
  sentence_lengths = [len(word_tokenize(sentence)) for sentence in sent_tokenizer(text_file)]
  # counting average sentence length
  avg_sent_len=sum(sentence_lengths) / (len(sentence_lengths)+0.000001)
  # getting percentage of complex words 
  percentage_complex_words = complex_word_count(text_file) / (len(tokenizer(text_file))+0.0000001)
  # getting fog index from average length and percentage of complex word words
  fog_index = 0.4 * (avg_sent_len + percentage_complex_words)
  return avg_sent_len,percentage_complex_words,fog_index

def average_number_of_words_per_sentence (text_file):
  # getting value from tokenizer,sent tokenizer
  the_number_of_words , the_number_of_sentences=len(tokenizer(text_file)),len(sent_tokenizer(text_file))
  return the_number_of_words / (the_number_of_sentences+0.000001)

import string
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

def word_count(text_file):
    # Remove punctuation from the text
    text_file = text_file.translate(str.maketrans('', '', string.punctuation))
    # Tokenize the text into individual words
    words = nltk.word_tokenize(text_file)
    # Remove stop words from the list of words
    stop_words = set(stopwords.words('english'))
    words = [word for word in words if word.upper() not in stop_words]
    
    # Return the count of words
    return len(words)

def syllable_count(word):
    # Remove trailing "es" and "ed" from the word
    if word.endswith("es"):
        word = word[:-2]
    elif word.endswith("ed"):
        word = word[:-2]
    
    # Count the number of vowels in the word
    vowels = "aeiouyAEIOUY"
    count = sum(word.count(vowel) for vowel in vowels)
    
    # Subtract one count for each silent e at the end of the word
    if word.endswith("e") and not word.endswith("le"):
        count -= 1
    
    # Ensure that the count is at least one
    if count < 1:
        count = 1
    
    return count

def syllable_count_per_word(text_file):
    # Tokenize the text into individual words
    words = nltk.word_tokenize(text_file)
    
    # Count the number of syllables in each word
    syllable_counts = sum([syllable_count(word) for word in words])
    
    # Return the list of syllable counts
    return syllable_counts/(len(words)+0.000001)

def personal_pronoun_count(text_file):
    # Define the regular expression for matching personal pronouns
    pattern = r"\b(I|we|my|ours|us)\b(?<!\bUS\b)"
    
    # Find all matches of the pattern in the text
    matches = re.findall(pattern, text_file)
    
    # Return the count of matches
    return len(matches)

def average_word_length(text_file):
  # split the text file in to list words
  words=text_file.split()
  # sum of all char in the word from words list
  total_chars=sum([len(word)for word in words])
  # length of list words
  total_words=len(words)
  return total_chars/(total_words+0.000001)

# creating all columns  that require putting their valuue zero
data['POSITIVE SCORE'],data['NEGATIVE SCORE'],data['POLARITY SCORE'],data['SUBJECTIVITY SCORE']=0,0,0,0
data['COMPLEX WORD COUNT']=0
data['AVERAGE SENTENCE LENGTH'],data['PERCENTAGE COMPLEX WORDS'],data['FOG INDEX']=0,0,0
data['AVERAGE NUMBER OF WORDS PER SENTENCE']=0
data['WORD COUNT']=0
data['SYLLABE COUNT PER WORD']=0
data['PERSONAL PRONOUN COUNT']=0
data['AVERAGE WORD LENGTH']=0

#  putting values into the columns using the functions 
for tokens,i in zip(text_filex_tokens,range(data.shape[0])):
  data['POSITIVE SCORE'][i],data['NEGATIVE SCORE'][i],data['POLARITY SCORE'][i],data['SUBJECTIVITY SCORE'][i]=variables(tokens)


for text,k in zip(text_filex,range(data.shape[0])):
  data['COMPLEX WORD COUNT'][k]=complex_word_count(text)
  data['AVERAGE SENTENCE LENGTH'][k],data['PERCENTAGE COMPLEX WORDS'][k],data['FOG INDEX'][k]=analysis_of_readibility(text)
  data['AVERAGE NUMBER OF WORDS PER SENTENCE'][k]=average_number_of_words_per_sentence (text)
  data['WORD COUNT'][k]=word_count(text)
  data['SYLLABE COUNT PER WORD'][k]=syllable_count_per_word(text)
  data['PERSONAL PRONOUN COUNT'][k]=personal_pronoun_count(text)
  data['AVERAGE WORD LENGTH'][k]=average_word_length(text)

# saving data as OUTPUT.xlsx excel file
data.to_excel('/Blackcoffer/Assignment/OUTPUT.xlsx')

!zip -r /Blackcoffer.zip /Blackcoffer

from google.colab import files
files.download('/Blackcoffer.zip')

